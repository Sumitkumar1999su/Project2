# importing library

import os
import numpy as np
import pandas as pd
import seaborn as sns


# Increase the print output

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)


os.chdir("C:\\Users\\hp\\Documents\\DaTa sci\\Project Data\\Finance DS")

# Read the data
fullRaw=pd.read_csv("finance_train.csv")
fullRaw.head()



fullRaw.head()
# Check for NAs
train.isnull().sum()

# Let us check the counts of 1's & 2's count in dataset
fullRaw['Revenue.Grid'].value_counts()

# % Split of 1s and 2s
fullRaw['Revenue.Grid'].value_counts()/ fullRaw.shape[0]

#fullRaw['Revenue.Grid'].value_counts(normalize=True)

# Summarize the data
fullRaw_Summary = fullRaw.describe()
# fullRaw_Summary = fullRaw.describe(include = "all")
fullRaw_Summary

# Lets drop 'Customer ID' column from the data as it is not going to assist us in our model
fullRaw.drop(['REF_NO'], axis = 1, inplace = True) 
fullRaw.shape



# Categorical variables: Gender, Academic_Qualification, Marital


variableToUpdate = 'children'
fullRaw[variableToUpdate].value_counts() # To check the unique categories of the variable
fullRaw[variableToUpdate].replace({1:"Single Child", 
                                     2:"Two Children",
                                     3:"Three Children",
                                     'Zero':"No Child"},inplace=True)
fullRaw[variableToUpdate].value_counts()

# Dummy variable creation
############################
fullRaw2 = pd.get_dummies(fullRaw, drop_first = True) # 'Source'  column will change to 'Source_Train' and it contains 0s and 1s
fullRaw2.shape

# Add Intercept Column
############################

# In Python, logistic regression function does NOT account for an intercept.
# So, we need to specify a column which has a constant value of 1 
from statsmodels.api import add_constant
fullRaw2 = add_constant(fullRaw2)
fullRaw2.shape

# VIF check
#########################
from statsmodels.stats.outliers_influence import variance_inflation_factor

tempMaxVIF = 10 # This VIF variable will be calculated at EVERY iteration in the while loop
maxVIF = 10
fullRawCopy = fullRaw2.copy()
counter = 1
highVIFColumnNames = []
while (tempMaxVIF >= maxVIF):
    
    print(counter)
    
    # Create an empty temporary df to store VIF values
    tempVIFDf = pd.DataFrame()
    
    # Calculate VIF using list comprehension
    tempVIFDf['VIF'] = [variance_inflation_factor(fullRaw2.values, i) for i in range(fullRaw2.shape[1])]
    
    # Create a new column "Column_Name" to store the col names against the VIF values from list comprehension
    tempVIFDf['Column_Name'] = fullRaw2.columns
    
    # Drop NA rows from the df - If there is some calculation error resulting in NAs
    tempVIFDf.dropna(inplace=True)
    
    # Sort the df based on VIF values, then pick the top most column name (which has the highest VIF)
    tempColumnName = tempVIFDf.sort_values(["VIF"], ascending = False).iloc[0,1]
    
    # Store the max VIF value in tempMaxVIF
    tempMaxVIF = tempVIFDf.sort_values(["VIF"], ascending = False).iloc[0,0]
    
    if (tempMaxVIF >= maxVIF): # This condition will ensure that columns having VIF lower than 10 are NOT dropped
        
        # Remove the highest VIF valued "Column" from trainXCopy. As the loop continues this step will keep removing highest VIF columns one by one 
        fullRaw2 = fullRaw2.drop(tempColumnName, axis = 1)    
        highVIFColumnNames.append(tempColumnName)
        print(tempColumnName)
    
    counter = counter + 1

highVIFColumnNames


highVIFColumnNames.remove('const') # We need to exclude 'const' column 
highVIFColumnNames
